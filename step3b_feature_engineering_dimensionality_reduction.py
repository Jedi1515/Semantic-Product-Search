# -*- coding: utf-8 -*-
"""Step_3B_Feature_Engineering_and_Dimensionality_Reduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XlJncIYbcNr9uCnVNsvJNDHEepqHctCN

# Step 3B — Feature Engineering & Dimensionality Reduction
Submitted by Jayesh Reddy

This notebook implements **Step 3B** of the capstone project. It constructs two parallel product representations for retrieval:

- **Baseline :** TF-IDF vectors over a unified `retrieval_text` field (Title + Description)
- **Improved :** Lightweight SentenceTransformer embeddings (`all-MiniLM-L6-v2`)

Followed by **PCA**  and a **2D visualisation** (UMAP) to inspect structure.

**Inputs**
- `walmart_products_clean_step3a.csv` (output of Step 3A)

**Outputs (saved to `../artifacts/`)**
- `tfidf_vectorizer.joblib`
- `tfidf_matrix.npz`
- `product_embeddings_allMiniLM_L6_v2.npy`
- `product_index.csv`
- `pca_full_on_embeddings.joblib`

## 1) Setup & Load Step 3A Output

This cell supports **Google Colab** (manual upload) and local execution (project path).
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ---- Detect environment ----
IN_COLAB = False
try:
    from google.colab import files  # Only available in Colab
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

# ---- Directories for artefacts ----
ARTIFACT_DIR = "../artifacts"
os.makedirs(ARTIFACT_DIR, exist_ok=True)

# ---- Load cleaned dataset from Step 3A ----
if IN_COLAB:
    print("Running in Google Colab.")
    print("Please upload the cleaned Step 3A dataset:")
    print("Expected file: walmart_products_clean_step3a.csv")

    uploaded = files.upload()
    filename = list(uploaded.keys())[0]
    print(f"Loaded file: {filename}")

    df = pd.read_csv(filename)
else:
    print("Running locally.")
    DATA_PATH = "../data/processed/walmart_products_clean_step3a.csv"
    df = pd.read_csv(DATA_PATH)

# ---- Check ----
print("Dataset shape:", df.shape)
df.head(3)

"""## 2) Construct Retrieval Text (Title + Description)

Build a single text field representing what the retrieval system “sees”.
"""

# --- Construct retrieval text (title + description) ---
required_cols = ["title_clean", "description_clean"]
missing_cols = [c for c in required_cols if c not in df.columns]
if missing_cols:
    raise ValueError(f"Missing required columns from Step 3A output: {missing_cols}")

df["retrieval_text"] = (
    df["title_clean"].fillna("") + " " + df["description_clean"].fillna("")
).str.strip()

empty_count = (df["retrieval_text"] == "").sum()
print("Empty retrieval_text rows:", empty_count)

# human  check
cols_to_show = [c for c in ["uniq_id", "title_clean", "description_clean", "retrieval_text"] if c in df.columns]
df.loc[:, cols_to_show].sample(3, random_state=42)

"""## 3) TF-IDF Baseline Representation


"""

from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
from scipy import sparse

# ---- TF-IDF configuration (baseline) ----
tfidf_vectorizer = TfidfVectorizer(
    lowercase=True,
    stop_words="english",
    ngram_range=(1, 2),
    min_df=2,        # ignore extremely rare terms
    max_df=0.95      # ignore extremely common terms
    # max_features=200000  # optional cap if needed; enable only if memory becomes an issue
)

# ---- Fit + transform ----
X_tfidf = tfidf_vectorizer.fit_transform(df["retrieval_text"])

print("TF-IDF matrix shape:", X_tfidf.shape)
print("Number of features:", len(tfidf_vectorizer.get_feature_names_out()))

# ---- Check: inspect top terms for a few products ----
feature_names = tfidf_vectorizer.get_feature_names_out()

def top_terms_for_row(X, row_idx, top_n=10):
    row = X[row_idx].tocsr()
    indices = row.indices
    data = row.data
    if len(indices) == 0:
        return []
    top_idx = np.argsort(data)[::-1][:top_n]
    return [(feature_names[indices[i]], float(data[i])) for i in top_idx]

for idx in df.sample(3, random_state=1).index:
    print("\n--- Example product ---")
    print("Title:", df.loc[idx, "title_clean"])
    terms = top_terms_for_row(X_tfidf, df.index.get_loc(idx), top_n=10)
    print("Top TF-IDF terms:", terms)

# ---- Save artefacts for Step 4 ----
VEC_PATH = os.path.join(ARTIFACT_DIR, "tfidf_vectorizer.joblib")
MAT_PATH = os.path.join(ARTIFACT_DIR, "tfidf_matrix.npz")

joblib.dump(tfidf_vectorizer, VEC_PATH)
sparse.save_npz(MAT_PATH, X_tfidf)

print("\nSaved vectorizer to:", VEC_PATH)
print("Saved TF-IDF matrix to:", MAT_PATH)

"""## 4) Semantic Embeddings (Lightweight SentenceTransformer)

Generate dense semantic vectors using  lightweight model.
"""

# Install / import (Colab-safe)
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    !pip -q install sentence-transformers
    from sentence_transformers import SentenceTransformer

import numpy as np

import os
import pandas as pd

# ---- Check we have retrieval_text ----
if "retrieval_text" not in df.columns:
    raise ValueError("Expected df['retrieval_text'] to exist. Run Section 2 first.")

texts = df["retrieval_text"].fillna("").tolist()

# ---- Lightweight sentence transformer ----
MODEL_NAME = "all-MiniLM-L6-v2"
model = SentenceTransformer(MODEL_NAME)

# ---- Encode ----
# normalize_embeddings=True gives L2-normalised embeddings suitable for cosine similarity
embeddings = model.encode(
    texts,
    batch_size=64,
    show_progress_bar=True,
    convert_to_numpy=True,
    normalize_embeddings=True
)

print("Embeddings shape:", embeddings.shape)
print("Embedding dtype:", embeddings.dtype)

# ---- Save embeddings ----
EMB_PATH = os.path.join(ARTIFACT_DIR, "product_embeddings_allMiniLM_L6_v2.npy")
np.save(EMB_PATH, embeddings)
print("Saved embeddings to:", EMB_PATH)

# ---- Save an index mapping (critical for Step 4) ----
index_cols = [c for c in ["uniq_id", "title_clean", "category", "brand", "price", "product_url"] if c in df.columns]
index_df = df[index_cols].copy()

IDX_PATH = os.path.join(ARTIFACT_DIR, "product_index.csv")
index_df.to_csv(IDX_PATH, index=False)
print("Saved product index to:", IDX_PATH)

# ---- Quick  check: norms should be ~1.0 due to normalisation ----
norms = np.linalg.norm(embeddings[:5], axis=1)
print("First 5 embedding norms:", norms)

index_df.head(3)

"""## 5) PCA on Embeddings


"""

from sklearn.decomposition import PCA
import joblib
import numpy as np
import os

# ---- PCA: fit on full embedding matrix ----
pca_full = PCA(random_state=42)
pca_full.fit(embeddings)

cum_var = np.cumsum(pca_full.explained_variance_ratio_)

# Print checkpoints for the write-up
for threshold in [0.50, 0.70, 0.80, 0.90, 0.95]:
    n_comp = int(np.argmax(cum_var >= threshold) + 1)
    print(f"Components for {int(threshold*100)}% variance: {n_comp}")

print("\nFirst 10 explained variance ratios:")
print(pca_full.explained_variance_ratio_[:10])

# ---- Save PCA model ----
PCA_PATH = os.path.join(ARTIFACT_DIR, "pca_full_on_embeddings.joblib")
joblib.dump(pca_full, PCA_PATH)
print("\nSaved PCA model to:", PCA_PATH)

"""## 6) 2D Visualisation (UMAP preferred)

This step is for *visual inspection* of the embedding space. We project a subset of products to 2D and colour by top categories.

> If UMAP isn't installed, the cell falls back to t-SNE as a backup plan
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# ---- Sample for visualisation (keeps runtime manageable) ----
n = len(df)
sample_size = min(2000, n)
rng = np.random.default_rng(42)
sample_idx = rng.choice(n, size=sample_size, replace=False)

emb_sample = embeddings[sample_idx]

# Category labels (optional, but useful)
if "category" in df.columns:
    cat_sample = df.iloc[sample_idx]["category"].fillna("Unknown").astype(str)
    top_cats = cat_sample.value_counts().head(10).index.tolist()
    cat_plot = cat_sample.where(cat_sample.isin(top_cats), other="Other")
else:
    cat_plot = pd.Series(["All"] * sample_size)

# ---- Try UMAP (preferred), fallback to t-SNE ----
use_umap = True
try:
    import umap  # umap-learn
except ImportError:
    use_umap = False

if use_umap:
    reducer = umap.UMAP(
        n_neighbors=15,
        min_dist=0.1,
        metric="cosine",
        random_state=42
    )
    emb_2d = reducer.fit_transform(emb_sample)
    method = "UMAP"
else:
    from sklearn.manifold import TSNE
    reducer = TSNE(
        n_components=2,
        perplexity=30,
        learning_rate="auto",
        init="random",
        random_state=42
    )
    emb_2d = reducer.fit_transform(emb_sample)
    method = "t-SNE"

# ---- Plot ----
plt.figure(figsize=(10, 7))
for cat in sorted(cat_plot.unique()):
    mask = (cat_plot == cat).to_numpy()
    plt.scatter(emb_2d[mask, 0], emb_2d[mask, 1], s=10, alpha=0.7, label=cat)

plt.title(f"2D visualisation of product embeddings ({method}, sample={sample_size})")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(bbox_to_anchor=(1.04, 1), loc="upper left")
plt.tight_layout()
plt.show()

"""## Generates Artefacts for Download"""

import os
import zipfile

ARTIFACT_DIR = "../artifacts"
BUNDLE_PATH = os.path.join(ARTIFACT_DIR, "step3b_artifacts_bundle.zip")

files_to_bundle = [
    "tfidf_vectorizer.joblib",
    "tfidf_matrix.npz",
    "product_embeddings_allMiniLM_L6_v2.npy",
    "product_index.csv",
    "pca_full_on_embeddings.joblib",
]

missing = [f for f in files_to_bundle if not os.path.exists(os.path.join(ARTIFACT_DIR, f))]
if missing:
    raise FileNotFoundError(f"Missing artefacts in {ARTIFACT_DIR}: {missing}")

with zipfile.ZipFile(BUNDLE_PATH, "w", compression=zipfile.ZIP_DEFLATED) as zf:
    for f in files_to_bundle:
        zf.write(os.path.join(ARTIFACT_DIR, f), arcname=f)

print("Created artefact bundle:", BUNDLE_PATH)

"""# Allows for download of artefacts .zip file

If you're working in a non-persistent environment this cell will allow for download of the Artefacts .zip file which would need to be uploaded in Step 4 (if you're in a non-persistent environment)
"""

import os

BUNDLE_PATH = "../artifacts/step3b_artifacts_bundle.zip"

if not os.path.exists(BUNDLE_PATH):
    raise FileNotFoundError(f"Expected artefact bundle not found at: {BUNDLE_PATH}")

if os.path.exists("/content"):
    try:
        from google.colab import files
        print("Detected Google Colab environment.")
        files.download(BUNDLE_PATH)
    except ImportError:
        print("Colab not detected. Artefact bundle available at:")
        print(os.path.abspath(BUNDLE_PATH))
else:
    print("Persistent environment detected.")
    print("Artefact bundle available at:")
    print(os.path.abspath(BUNDLE_PATH))