# -*- coding: utf-8 -*-
"""Step_3A_Preprocessing_and_EDA_Walmart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VWJR5bydn5wpkugSx8kpQWXq5Atteb95

# Step 3A â€” Preprocessing and Applied EDA

Submitted by Jayesh Reddy

This notebook implements **Step 3A** of the capstone: preprocessing and applied EDA.

**Outputs:**
- Cleaned dataset saved to `../data/processed/walmart_products_clean_step3a.csv`
- EDA plots (text lengths, category/brand distributions, price distributions)
"""

import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# 0) Setup + Load Data"""

import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ---- Environment detection ----
IN_COLAB = False
try:
    from google.colab import files  # Only available in Colab
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

# ---- Output directory (used later) ----
OUT_DIR = "../data/processed"
os.makedirs(OUT_DIR, exist_ok=True)

# ---- Load dataset ----
if IN_COLAB:
    print("Running in Google Colab.")
    print("Please upload the Walmart dataset CSV file...")
    uploaded = files.upload()

    # Get uploaded filename
    filename = list(uploaded.keys())[0]
    print(f"Loaded file: {filename}")

    df_raw = pd.read_csv(filename, low_memory=False)
else:
    print("Running locally.")
    DATA_PATH = "../data/raw/marketing_sample_for_walmart_com-ecommerce__20191201_20191231__30k_data.csv"
    df_raw = pd.read_csv(DATA_PATH, low_memory=False)

# ---- Quick sanity check ----
print("Dataset shape:", df_raw.shape)
print("Columns:", list(df_raw.columns))
df_raw.head(3)

"""# 1) Standardise Column Names"""

def to_snake_case(s: str) -> str:
    s = s.strip()
    s = re.sub(r"[^\w\s]", "", s)          # remove punctuation
    s = re.sub(r"\s+", "_", s)             # spaces -> underscore
    return s.lower()

df = df_raw.copy()
df.columns = [to_snake_case(c) for c in df.columns]

print("Standardised columns:", list(df.columns))
df.head(3)

"""# 2) Basic Data Quality Checks (Nulls, Types, Duplicates)"""

print("Dtypes:\n", df.dtypes)

# Missing values summary
missing = df.isna().mean().sort_values(ascending=False) * 100
print("\nMissing values (%):\n", missing)

# Duplicate rows (full row duplicates)
dup_full = df.duplicated().sum()
print("\nFull-row duplicates:", dup_full)

# Duplicate products (heuristic: same product_name + description)
if "product_name" in df.columns and "description" in df.columns:
    dup_prod = df.duplicated(subset=["product_name", "description"]).sum()
    print("Duplicate product_name+description:", dup_prod)

"""# 3) Minimal Text Cleaning

Keep meaning intact
Light-touch cleaning: remove extra whitespace and common boilerplate patterning, without aggressive NLP preprocessing

"""

TEXT_COLS = ["product_name", "description", "category", "brand"]

for c in TEXT_COLS:
    if c in df.columns:
        df[c] = df[c].astype(str)

def clean_text_basic(s: str) -> str:
    s = s.replace("\u00a0", " ")                 # non-breaking spaces
    s = re.sub(r"\s+", " ", s).strip()           # collapse whitespace
    return s

if "description" in df.columns:
    df["description_clean"] = df["description"].apply(clean_text_basic)
if "product_name" in df.columns:
    df["title_clean"] = df["product_name"].apply(clean_text_basic)

df[["product_name", "title_clean", "description", "description_clean"]].head(3)

"""# 4) Handle Missing/Empty Text

items with no usable text to be removed or flagged
"""

def is_blank(x: str) -> bool:
    return (x is None) or (str(x).strip() == "") or (str(x).strip().lower() == "nan")

df["title_blank"] = df["title_clean"].apply(is_blank) if "title_clean" in df.columns else True
df["desc_blank"]  = df["description_clean"].apply(is_blank) if "description_clean" in df.columns else True

before = len(df)
df = df[~(df["title_blank"] & df["desc_blank"])].copy()
after = len(df)

print(f"Removed rows with BOTH blank title and blank description: {before-after}")
print("New shape:", df.shape)

"""#5) Remove Duplicates"""

before = len(df)
if "title_clean" in df.columns and "description_clean" in df.columns:
    df = df.drop_duplicates(subset=["title_clean", "description_clean"]).copy()
after = len(df)

print(f"Removed duplicates by (title_clean, description_clean): {before-after}")
print("New shape:", df.shape)

"""# 6) Price Cleaning + Outlier Flagging

The aim is not to delete outliers. Need to understand for analysis
"""

PRICE_COLS = [c for c in ["list_price", "sale_price"] if c in df.columns]

for c in PRICE_COLS:
    df[c] = pd.to_numeric(df[c], errors="coerce")

# Choose a working price field: sale_price if present else list_price
if "sale_price" in df.columns:
    df["price"] = df["sale_price"]
elif "list_price" in df.columns:
    df["price"] = df["list_price"]
else:
    df["price"] = np.nan

# IQR outlier flags (for discussion)
price_nonnull = df["price"].dropna()
q1, q3 = price_nonnull.quantile([0.25, 0.75])
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

df["price_outlier_iqr"] = (df["price"] < lower) | (df["price"] > upper)

print("Price null %:", df["price"].isna().mean() * 100)
print("IQR bounds:", lower, upper)
print("Outliers flagged (%):", df["price_outlier_iqr"].mean() * 100)

"""#7 Applied EDA - Text Length Distributions

"""

df["title_len"] = df["title_clean"].str.len()
df["desc_len"]  = df["description_clean"].str.len()

plt.figure()
plt.hist(df["title_len"].dropna(), bins=50)
plt.title("Title length distribution")
plt.xlabel("Characters")
plt.ylabel("Count")
plt.show()

plt.figure()
plt.hist(df["desc_len"].dropna(), bins=50)
plt.title("Description length distribution")
plt.xlabel("Characters")
plt.ylabel("Count")
plt.show()

print("Title length (chars):\n", df["title_len"].describe())
print("\nDescription length (chars):\n", df["desc_len"].describe())

"""#Applied EDA - Category Distribution"""

if "category" in df.columns:
    top_cats = df["category"].value_counts().head(20)
    plt.figure(figsize=(10, 6))
    plt.barh(top_cats.index[::-1], top_cats.values[::-1])
    plt.title("Top 20 categories by product count")
    plt.xlabel("Count")
    plt.ylabel("Category")
    plt.tight_layout()
    plt.show()

    print("Unique categories:", df["category"].nunique())

"""#Applied EDA - Brand Distribution"""

if "brand" in df.columns:
    top_brands = df["brand"].value_counts().head(20)
    plt.figure(figsize=(10, 6))
    plt.barh(top_brands.index[::-1], top_brands.values[::-1])
    plt.title("Top 20 brands by product count")
    plt.xlabel("Count")
    plt.ylabel("Brand")
    plt.tight_layout()
    plt.show()

    print("Unique brands:", df["brand"].nunique())

"""# Applied EDA - Price Distribution"""

plt.figure()
plt.hist(df["price"].dropna(), bins=60)
plt.title("Price distribution (raw)")
plt.xlabel("Price")
plt.ylabel("Count")
plt.show()

# Optional: log-scale view (often more interpretable for retail prices)
price_pos = df.loc[df["price"] > 0, "price"]
plt.figure()
plt.hist(np.log1p(price_pos), bins=60)
plt.title("Price distribution (log1p)")
plt.xlabel("log1p(price)")
plt.ylabel("Count")
plt.show()

print(df["price"].describe())
print("Outliers flagged (count):", df["price_outlier_iqr"].sum())

# Show a few random products to check the cleaned text
cols_to_show = [c for c in ["uniq_id", "title_clean", "description_clean", "category", "brand", "price"] if c in df.columns]
df.sample(5, random_state=42)[cols_to_show]

"""# Save clean dataset for subsequent use"""

import os

OUT_DIR = "/content/data/processed"
os.makedirs(OUT_DIR, exist_ok=True)

OUT_PATH = os.path.join(OUT_DIR, "walmart_products_clean_step3a.csv")
df.to_csv(OUT_PATH, index=False)

print("Saved to:", OUT_PATH)

"""# Allows download of cleaned dataset

If you're working in a non-persistent environment this cell will allow for a download of the cleaned dataset here which would be uploaded in subsequent notebook (if you're in a non-persistent environment)
"""

import os

OUT_PATH = "/content/data/processed/walmart_products_clean_step3a.csv"

if os.path.exists("/content"):
    try:
        from google.colab import files
        print("Detected Google Colab environment.")
        files.download(OUT_PATH)
    except ImportError:
        print("Not running in Colab. File saved locally at:")
        print(OUT_PATH)
else:
    print("Persistent environment detected.")
    print("File available at:")
    print(OUT_PATH)