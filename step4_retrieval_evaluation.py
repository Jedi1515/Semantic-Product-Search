# -*- coding: utf-8 -*-
"""Step_4_Retrieval_and_Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17YeCusZxovjJYMhIh_-eMIibU_0WUgpk

# Step 4 — Retrieval & Evaluation (Semantic Search)

Submitted by Jayesh Reddy

This notebook implements **Step 4** of the machine learning lifecycle for the project.

Focus of this step:
- Implement retrieval models (TF-IDF baseline vs semantic embeddings)
- Evaluate retrieval quality using **category-based ranking metrics**
- Complement quantitative evaluation with **manual, human-interpretable examples**

The notebook is intentionally **heavily commented** to clarify design choices and
support assessors reviewing the logic step-by-step.

## 1) Load artefacts from Step 3B (environment-aware)
"""

# ------------------------------------------------------------
# This cell loads all artefacts produced in Step 3B.
# The workflow is artefact-based rather than state-based:
#   - Step 3B produces feature representations and saves them
#   - Step 4 consumes those artefacts for retrieval & evaluation
#
# The logic below supports both:
#   - Local Jupyter environments (persistent filesystem)
#   - Google Colab (ephemeral filesystem, manual upload required)
# ------------------------------------------------------------

import os
import zipfile
import numpy as np
import pandas as pd
import joblib
from scipy import sparse

# ---- Detect whether we are running in Google Colab ----
IN_COLAB = False
try:
    from google.colab import files
    IN_COLAB = True
except ImportError:
    pass

# ---- Artefact directory ----
ARTIFACT_DIR = "../artifacts"
os.makedirs(ARTIFACT_DIR, exist_ok=True)

# ---- If in Colab, upload and extract artefact bundle ----
if IN_COLAB:
    print("Running in Google Colab.")
    print("Please upload: step3b_artifacts_bundle.zip")
    uploaded = files.upload()
    zip_name = list(uploaded.keys())[0]

    with zipfile.ZipFile(zip_name, "r") as zf:
        zf.extractall(ARTIFACT_DIR)

    print("Artefacts extracted to:", ARTIFACT_DIR)
else:
    print("Running locally. Expecting artefacts in:", ARTIFACT_DIR)

# ---- Load artefacts ----
# TF-IDF artefacts (baseline lexical representation)
tfidf_vectorizer = joblib.load(os.path.join(ARTIFACT_DIR, "tfidf_vectorizer.joblib"))
X_tfidf = sparse.load_npz(os.path.join(ARTIFACT_DIR, "tfidf_matrix.npz"))

# Semantic embedding artefacts
embeddings = np.load(os.path.join(ARTIFACT_DIR, "product_embeddings_allMiniLM_L6_v2.npy"))

# Product metadata index (titles, categories, etc.)
product_index = pd.read_csv(os.path.join(ARTIFACT_DIR, "product_index.csv"))

# ---- Sanity checks ----
print("TF-IDF matrix shape:", X_tfidf.shape)
print("Embedding matrix shape:", embeddings.shape)
print("Product index shape:", product_index.shape)

product_index.head(3)

"""## 2) Helper utilities"""

# ------------------------------------------------------------
# This cell defines helper utilities used throughout Step 4.
#
# For display and safety
# ------------------------------------------------------------

from typing import List

def safe_get(col, default=""):
    """
    Safely retrieve a column from product_index.
    If the column does not exist, return a default-filled Series.
    """
    if col in product_index.columns:
        return product_index[col].fillna(default).astype(str)
    return pd.Series([default] * len(product_index))

# Core metadata used in evaluation and display
titles = safe_get("title_clean")
categories = safe_get("category", default="Unknown")
brands = safe_get("brand", default="Unknown")

# Optional numeric attribute
prices = product_index["price"] if "price" in product_index.columns else None

def show_results(indices: List[int], scores: List[float], header: str = ""):
    """
    Pretty-print ranked retrieval results for manual inspection.
    """
    print("\n" + header)
    for rank, (i, s) in enumerate(zip(indices, scores), start=1):
        line = f"{rank:02d}. {titles.iloc[i]} | category={categories.iloc[i]}"
        if prices is not None:
            try:
                line += f" | price={float(prices.iloc[i]):.2f}"
            except Exception:
                pass
        line += f" | score={s:.4f}"
        print(line)

"""## 3) Retrieval functions"""

# ------------------------------------------------------------
# Retrieval functions
#
# Both retrieval methods expose the same interface:
#   input  -> free-text query
#   output -> ranked list of product indices + similarity scores
#
# ------------------------------------------------------------

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def retrieve_tfidf(query: str, top_k: int = 10):
    """
    Retrieve products using TF-IDF cosine similarity.
    This represents a traditional keyword-based retrieval baseline.
    """
    # Vectorise query into sparse TF-IDF space
    q_vec = tfidf_vectorizer.transform([query])

    # Compute cosine similarity against all products
    sims = cosine_similarity(q_vec, X_tfidf).ravel()

    # Rank products by similarity
    idx = np.argsort(sims)[::-1][:top_k]
    return idx.tolist(), sims[idx].tolist()

def retrieve_embeddings(query: str, model, top_k: int = 10):
    """
    Retrieve products using semantic embeddings.
    Embeddings are L2-normalised, so dot product equals cosine similarity.
    """
    # Encode query into the same semantic space as products
    q_emb = model.encode(
        [query],
        convert_to_numpy=True,
        normalize_embeddings=True
    )

    # Cosine similarity via dot product
    sims = (q_emb @ embeddings.T).ravel()

    # Rank products by similarity
    idx = np.argsort(sims)[::-1][:top_k]
    return idx.tolist(), sims[idx].tolist()

"""## 4) Load sentence transformer model"""

# ------------------------------------------------------------
# Same pretrained sentence transformer used in Step 3B.
#
# ------------------------------------------------------------

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    !pip -q install sentence-transformers
    from sentence_transformers import SentenceTransformer

MODEL_NAME = "all-MiniLM-L6-v2"
st_model = SentenceTransformer(MODEL_NAME)

print("Loaded embedding model:", MODEL_NAME)

"""## 5) Manual  qualitative check

"""

# ------------------------------------------------------------
# Compare TF-IDF vs embeddings on a small set of natural-language queries.
# Not a formal evaluation — just validates that retrieval works.
# ------------------------------------------------------------

test_queries = [
    "small quiet blender for smoothies easy to clean",
    "wireless security camera for baby monitor with night vision",
    "solar powered bicycle light with horn and USB charging",
]

for q in test_queries:
    tf_idx, tf_sc = retrieve_tfidf(q, top_k=5)
    em_idx, em_sc = retrieve_embeddings(q, st_model, top_k=5)

    print("\n" + "=" * 90)
    print("QUERY:", q)
    show_results(tf_idx, tf_sc, header="TF-IDF (lexical baseline) — Top 5")
    show_results(em_idx, em_sc, header="Embeddings (semantic retrieval) — Top 5")

"""## 6) Build category-based evaluation set"""

# ------------------------------------------------------------
# Category-based evaluation setup
#
# ------------------------------------------------------------

import numpy as np

if "category" not in product_index.columns:
    raise ValueError("Category column is required for category-based evaluation")

# Number of target products to evaluate
n_targets = 300
rng = np.random.default_rng(42)

# Prefer products with known categories
valid_mask = (categories != "Unknown")
valid_indices = np.where(valid_mask.to_numpy())[0]

target_indices = rng.choice(
    valid_indices,
    size=min(n_targets, len(valid_indices)),
    replace=False
)

# Queries  derived from product titles
queries = titles.iloc[target_indices].tolist()
target_categories = categories.iloc[target_indices].tolist()

# Build mapping: category -> product indices
cat_to_indices = {}
for i, c in enumerate(categories.tolist()):
    cat_to_indices.setdefault(c, []).append(i)

print("Evaluation targets:", len(target_indices))

"""## 7) Ranking metrics"""

# ------------------------------------------------------------
# Ranking metrics  used in information retrieval:
# - Precision@K
# - Recall@K
# - nDCG@K
# ------------------------------------------------------------

import math

def precision_at_k(retrieved, relevant, k):
    return sum(1 for i in retrieved[:k] if i in relevant) / k

def recall_at_k(retrieved, relevant, k):
    return sum(1 for i in retrieved[:k] if i in relevant) / max(1, len(relevant))

def ndcg_at_k(retrieved, relevant, k):
    score = 0.0
    for rank, item in enumerate(retrieved[:k], start=1):
        rel = 1 if item in relevant else 0
        score += (2**rel - 1) / math.log2(rank + 1)

    ideal = min(k, len(relevant))
    idcg = sum((2**1 - 1) / math.log2(r + 1) for r in range(1, ideal + 1))
    return score / idcg if idcg > 0 else 0.0

"""## 8) Run quantitative evaluation"""

# ------------------------------------------------------------
# Compute average retrieval performance for TF-IDF vs embeddings
# across multiple K values.
# ------------------------------------------------------------

from tqdm.auto import tqdm

Ks = [5, 10]
results = []

for q, idx, cat in tqdm(zip(queries, target_indices, target_categories)):
    # Relevant items: same category (excluding the query product itself)
    relevant = set(cat_to_indices.get(cat, []))
    if idx in relevant:
        relevant.remove(idx)

    tf_idx, _ = retrieve_tfidf(q, top_k=max(Ks))
    em_idx, _ = retrieve_embeddings(q, st_model, top_k=max(Ks))

    row = {}
    for k in Ks:
        row[f"tf_p@{k}"] = precision_at_k(tf_idx, relevant, k)
        row[f"tf_r@{k}"] = recall_at_k(tf_idx, relevant, k)
        row[f"tf_ndcg@{k}"] = ndcg_at_k(tf_idx, relevant, k)

        row[f"em_p@{k}"] = precision_at_k(em_idx, relevant, k)
        row[f"em_r@{k}"] = recall_at_k(em_idx, relevant, k)
        row[f"em_ndcg@{k}"] = ndcg_at_k(em_idx, relevant, k)

    results.append(row)

eval_df = pd.DataFrame(results)
eval_df.mean()

"""## 9) Manual qualitative evaluation
Based on fed-in queries
"""

# ------------------------------------------------------------
# Manual evaluation for comparison to quantitative metrics
# Aim to show results would appear to a user
# ------------------------------------------------------------

manual_queries = [
    "a gentle vanilla scented hand cream for dry skin",
    "bike headlight with a loud horn and solar charging",
    "wifi baby monitor camera with night vision and 1080p",
]

for q in manual_queries:
    tf_idx, tf_sc = retrieve_tfidf(q, top_k=5)
    em_idx, em_sc = retrieve_embeddings(q, st_model, top_k=5)

    print("\n" + "=" * 90)
    print("MANUAL QUERY:", q)
    show_results(tf_idx, tf_sc, header="TF-IDF — Top 5")
    show_results(em_idx, em_sc, header="Embeddings — Top 5")